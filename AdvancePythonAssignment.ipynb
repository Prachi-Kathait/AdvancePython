{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2sZr5Ko0p26"
   },
   "source": [
    "**Part I: Process Automation**\n",
    "\n",
    "Q1. Create a file that contains 1000 lines of random strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MunbAhKd0glx",
    "outputId": "70bf8019-9014-4fd0-9900-0bebda5e3fef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing completed\n"
     ]
    }
   ],
   "source": [
    "import random as r \n",
    "import string as s\n",
    "fp = open('result_Q1.txt','w');\n",
    "for i in range(1000):\n",
    "  random_string = r.sample(s.ascii_letters,r.randint(10,15));\n",
    "  random_string1 = \"\".join(random_string);\n",
    "  fp.write(str(random_string1)+\"\\n\");\n",
    "\n",
    "fp.close();\n",
    "print(\"writing completed\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0G-Q8Ahl2uJS"
   },
   "source": [
    "Q2. Create a file that contains multiple lines of random strings and file size must be 5 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jowhFVvT2ti0",
    "outputId": "a1244855-9fc3-4a9d-e4e9-c6b8ed9e8587"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing completed\n"
     ]
    }
   ],
   "source": [
    "import random as r \n",
    "import string as s\n",
    "fp = open('result_Q2.txt','w');\n",
    "target_file_size = 5 * 1024 * 1024;\n",
    "filesize = 0;\n",
    "while filesize<=target_file_size:\n",
    "  random_string = r.sample(s.ascii_letters,r.randint(10,15));\n",
    "  random_string1 = \"\".join(random_string);\n",
    "  line_size = len(random_string1)+1;\n",
    "  filesize += line_size;\n",
    "  fp.write(str(random_string1)+\"\\n\");\n",
    "\n",
    "fp.close();\n",
    "print(\"writing completed\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgzpHr5438r5"
   },
   "source": [
    "Q3. Create 10 files that contains multiple lines of random strings and file size of each file must be 5 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbmQXned4C03",
    "outputId": "362a7175-9d8e-4d7d-9b6c-35e014a7ccd9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing completed\n"
     ]
    }
   ],
   "source": [
    "import random as r \n",
    "import string as s\n",
    "target_file_size = 5 * 1024 * 1024;\n",
    "for i in range(10):\n",
    "  file_name = f'result_string_Q3_{i}.txt'\n",
    "  fp = open(file_name,'w');\n",
    "  filesize = 0;\n",
    "  while filesize<=target_file_size:\n",
    "    random_string = r.sample(s.ascii_letters,r.randint(10,15));\n",
    "    random_string1 = \"\".join(random_string);\n",
    "    line_size = len(random_string1)+1;\n",
    "    filesize += line_size;\n",
    "    fp.write(str(random_string1)+\"\\n\");\n",
    "  fp.close();\n",
    "\n",
    "print(\"writing completed\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkeojC1m51NW"
   },
   "source": [
    "Q4. Create 5 files of size 1GB, 2GB, 3GB, 4GB and 5GB; file contains multiple lines of random strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Z86qrEf53Y3",
    "outputId": "5f7c627c-aac9-40ab-e6cc-4d5dc4612d0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing completed\n"
     ]
    }
   ],
   "source": [
    "import random as r \n",
    "import string as s\n",
    "for i in range(5):\n",
    "  file_name = f'result_string_Q4_{i}.txt'\n",
    "  target_file_size = (i+1) * 1024 * 1024;\n",
    "  fp = open(file_name,'w');\n",
    "  filesize = 0;\n",
    "  while filesize<=target_file_size:\n",
    "    random_string = r.sample(s.ascii_letters,r.randint(10,15));\n",
    "    random_string1 = \"\".join(random_string);\n",
    "    line_size = len(random_string1)+1;\n",
    "    filesize += line_size;\n",
    "    fp.write(str(random_string1)+\"\\n\");\n",
    "  fp.close();\n",
    "\n",
    "print(\"writing completed\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1PoYHuK7jeW"
   },
   "source": [
    "Q5. Convert all the files of Q4 into upper case one by one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j90FWST27mfU",
    "outputId": "5987deaf-810a-4b43-b7a9-e26a42479eca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing completed\n"
     ]
    }
   ],
   "source": [
    "import string as s\n",
    "for i in range(5):\n",
    "  file_name2 = f'result_string_Q5_{i}.txt'\n",
    "  file_name1 = f'result_string_Q4_{i}.txt'\n",
    "  fp1 = open(file_name1);\n",
    "  fp2 = open(file_name2,'w');\n",
    "  for line in fp1:\n",
    "    fp2.write(line.upper());\n",
    "  fp1.close();\n",
    "  fp2.close();\n",
    "\n",
    "print(\"writing completed\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQz9f3Mh9SHw"
   },
   "source": [
    "Q6. Convert all the files of Q4 into upper case parallel using multi-threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T_OCwA6-9WMV",
    "outputId": "c3d06962-346b-465d-a223-09398f60c680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing completed\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import concurrent.futures\n",
    "\n",
    "def convert_to_uppercase(line):\n",
    "    \"\"\"Convert a line to uppercase\"\"\"\n",
    "    return line.upper()\n",
    "\n",
    "for i in range(5):\n",
    "    file_name2 = f'result_string_Q6_{i}.txt'\n",
    "    file_name1 = f'result_string_Q4_{i}.txt'\n",
    "    \n",
    "    with open(file_name1, 'r') as fp1, open(file_name2, 'w') as fp2:\n",
    "        lines = fp1.readlines()\n",
    "\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            # Convert lines to uppercase in parallel\n",
    "            converted_lines = executor.map(convert_to_uppercase, lines)\n",
    "\n",
    "            # Write converted lines to the new file\n",
    "            for converted_line in converted_lines:\n",
    "                fp2.write(converted_line)\n",
    "\n",
    "print(\"Writing completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rmb_WpdV_gxD"
   },
   "source": [
    "Q7. WAP to automatically download 10 images of cat from “Google Images”. [Hint: Find the package from \n",
    "pypi.org and use it]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JAX33AXhgQg7",
    "outputId": "5196bfc3-f79a-4bc7-a4e9-294ed837f895"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bing-image-downloader\n",
      "  Downloading bing_image_downloader-1.1.2-py3-none-any.whl (5.9 kB)\n",
      "Installing collected packages: bing-image-downloader\n",
      "Successfully installed bing-image-downloader-1.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bing-image-downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLUDBkjghFtK",
    "outputId": "096856a5-cced-44cc-8fc6-0168a64012dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[%] Downloading Images to C:\\Users\\DELL\\Downloads\\dataset\\cat\n",
      "\n",
      "\n",
      "[!!]Indexing page: 1\n",
      "\n",
      "[%] Indexed 10 Images on Page 1.\n",
      "\n",
      "===============================================\n",
      "\n",
      "[%] Downloading Image #1 from http://images6.fanpop.com/image/photos/36700000/Cats-image-cats-36712807-1600-1200.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #2 from http://www.toledoblade.com/image/2014/05/28/800x_b1_cCM_z/n4fletchercat.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #3 from https://www.ovrs.com/blog/wp-content/uploads/2014/12/iStock_000029861698_Medium-1.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #4 from https://novacatclinic.com/wp-content/uploads/2019/03/IMG_8668.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #5 from https://sgaissert.files.wordpress.com/2009/09/lilaccloseup.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #6 from https://3.bp.blogspot.com/-7ABAaAViPyw/W84wG4scmQI/AAAAAAAAFUU/7jbp2CidXKYRRH81GIFOeEa7-D3GabJqgCLcBGAs/s1600/cat.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #7 from http://tokyobling.files.wordpress.com/2010/02/cat_cafe_1.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #8 from https://get.pxhere.com/photo/animal-pet-kitten-cat-small-mammal-fauna-heal-blue-eye-close-up-nose-whiskers-vertebrate-domestic-lying-tabby-cat-norwegian-forest-cat-ginger-fur-small-to-medium-sized-cats-cat-like-mammal-carnivoran-domestic-short-haired-cat-domestic-long-haired-cat-609263.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #9 from https://novacatclinic.com/wp-content/uploads/2020/09/kitten-1-scaled.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "[%] Downloading Image #10 from http://upload.wikimedia.org/wikipedia/commons/d/d4/Cat_March_2010-1a.jpg\n",
      "[%] File Downloaded !\n",
      "\n",
      "\n",
      "\n",
      "[%] Done. Downloaded 10 images.\n"
     ]
    }
   ],
   "source": [
    "from bing_image_downloader import downloader\n",
    "\n",
    "downloader.download(\"cat\", limit=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDZsRdifH4DT"
   },
   "source": [
    "Q8. WAP to automatically download 10 videos of “Machine Learning” from “Youtube.com”. [Hint: Find the \n",
    "package from pypi.org and use it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9x6Z7DjebGln",
    "outputId": "ae84d6dd-5947-4aa3-e7c8-49703b179541"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytube\n",
      "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
      "     -------------------------------------- 57.6/57.6 kB 434.4 kB/s eta 0:00:00\n",
      "Installing collected packages: pytube\n",
      "Successfully installed pytube-15.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-z3O4amAcdlU",
    "outputId": "ad3ac173-69a4-4522-c196-de3eb26fc7ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading : Machine Learning\n"
     ]
    }
   ],
   "source": [
    "from pytube import Playlist\n",
    "\n",
    "py = Playlist(\"https://www.youtube.com/playlist?list=PLYwpaL_SFmcBhOEPwf5cFwqo5B-cP9G4P\")\n",
    "print(f'Downloading : {py.title}')\n",
    "i = 1;\n",
    "for video in py.videos:\n",
    "  if(i<=20):\n",
    "    video.streams.first().download()\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0GaJlkKI7lV"
   },
   "source": [
    "Q9. Convert all the videos of Q8 and convert it to audio. [Hint: Find the package from pypi.org and use it]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fjtFCt_mg6jd",
    "outputId": "8404dbe6-456a-4b8b-e8fe-55e524974543"
   },
   "outputs": [],
   "source": [
    "pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kU6cd62AhbuD",
    "outputId": "d266499d-379c-4be3-a6ac-3908c26f79dc"
   },
   "outputs": [],
   "source": [
    "#import library\n",
    "import moviepy.editor as movie\n",
    "#get video file\n",
    "videoClip = movie.VideoFileClip(\"Basics Of Principal Component Analysis Part-1 Explained in Hindi ll Machine Learning Course.3gpp\"\n",
    ")\n",
    "# copy the path of the file which you want to change to audio from content or download\n",
    "#convert it to audio file and save\n",
    "#here the generated audio filename is krazy\n",
    "videoClip.audio.write_audiofile(\"krazy.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YiBfmJVqKL_Y"
   },
   "source": [
    "Q10. Create an automated pipeline using multi-threading for:\n",
    "“Automatic Download of 100 Videos from YouTube” → “Convert it to Audio”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lrcL9h0FhcJC",
    "outputId": "8800acb6-e579-4a32-b82c-f7149444fceb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "from queue import Queue\n",
    "from subprocess import call\n",
    "\n",
    "# Number of threads to use for downloading and converting\n",
    "NUM_THREADS = 5\n",
    "\n",
    "# List of YouTube video URLs\n",
    "video_urls = [\n",
    "    \"https://www.youtube.com/watch?v=Y4qO9unerGs&list=PLYwpaL_SFmcBhOEPwf5cFwqo5B-cP9G4P&index=1&t=1s\",\n",
    "    \"https://www.youtube.com/watch?v=YHcAQKrh3E4&list=PLYwpaL_SFmcBhOEPwf5cFwqo5B-cP9G4P&index=2\",\n",
    "    # Add more video URLs here\n",
    "    # ...\n",
    "]\n",
    "\n",
    "# Output directory for downloaded videos and converted audio\n",
    "output_dir = \"output\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Create a queue for the video URLs\n",
    "video_queue = Queue()\n",
    "\n",
    "# Add the video URLs to the queue\n",
    "for url in video_urls:\n",
    "    video_queue.put(url)\n",
    "\n",
    "# Function to download and convert videos\n",
    "def process_videos():\n",
    "    while not video_queue.empty():\n",
    "        # Get the next video URL from the queue\n",
    "        video_url = video_queue.get()\n",
    "\n",
    "        # Download the video using youtube-dl\n",
    "        download_command = f\"youtube-dl -o '{output_dir}/%(id)s.%(ext)s' -f 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best' {video_url}\"\n",
    "        call(download_command, shell=True)\n",
    "\n",
    "        # Get the video ID\n",
    "        video_id = video_url.split(\"v=\")[1]\n",
    "\n",
    "        # Convert the video to audio using ffmpeg\n",
    "        video_file = f\"{output_dir}/{video_id}.mp4\"\n",
    "        audio_file = f\"{output_dir}/{video_id}.mp3\"\n",
    "        convert_command = f\"ffmpeg -i {video_file} {audio_file}\"\n",
    "        call(convert_command, shell=True)\n",
    "\n",
    "        # Delete the video file\n",
    "        os.remove(video_file)\n",
    "\n",
    "        # Mark the task as done in the queue\n",
    "        video_queue.task_done()\n",
    "\n",
    "# Create and start the threads\n",
    "threads = []\n",
    "for _ in range(NUM_THREADS):\n",
    "    thread = threading.Thread(target=process_videos)\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "# Wait for all the threads to finish\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "# All videos have been downloaded and converted\n",
    "print(\"All videos have been downloaded and converted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vyui7zLdKp5P"
   },
   "source": [
    "Q11. Create an automated pipeline using multi-threading for: “Automatic Download of 500 images of Dog from \n",
    "GoogleImages” → “Rescale it to 50%”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YxFWuj3Vpy4q",
    "outputId": "9214b31f-5f69-4cee-b722-23867b9d2ede"
   },
   "outputs": [],
   "source": [
    "pip install bing-image-downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_-SIH-VhdkU",
    "outputId": "34054ce0-63ab-4119-ff96-7e4ecf8fa618"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from bing_image_downloader import downloader\n",
    "from PIL import Image\n",
    "\n",
    "# Download the images using bing_image_downloader\n",
    "downloader.download(\"dog\", limit=50)\n",
    "\n",
    "# Set the directory where the images are downloaded\n",
    "download_dir = \"dataset/dog\"\n",
    "\n",
    "# Loop through the downloaded images\n",
    "for filename in os.listdir(download_dir):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        # Construct the full path to the image\n",
    "        image_path = os.path.join(download_dir, filename)\n",
    "\n",
    "        # Open the original image\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Get the original image dimensions\n",
    "        width, height = image.size\n",
    "\n",
    "        # Calculate the new dimensions with 50% scaling\n",
    "        new_width = int(width * 0.5)\n",
    "        new_height = int(height * 0.5)\n",
    "\n",
    "        # Resize the image using the new dimensions\n",
    "        resized_image = image.resize((new_width, new_height))\n",
    "\n",
    "        # Save the resized image with a new filename\n",
    "        resized_filename = \"resized_\" + filename\n",
    "        resized_image.save(resized_filename)\n",
    "\n",
    "        # Close the original and resized images\n",
    "        image.close()\n",
    "        resized_image.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Cie5JTCQWh6"
   },
   "source": [
    "Q12. Create a random dataset of 100 rows and 30 columns. All the values are defined between [1,200]. Perform \n",
    "the following operations:\n",
    "\n",
    "(i) Replace all the values with NA in the dataset defined between [10, 60]. Print the count of number \n",
    "rows having missing values.\n",
    "\n",
    "(ii) Replace all the NA values with the average of the column value. \n",
    "\n",
    "(iii) Find the Pearson correlation among all the columns and plot heat map. Also select those columns \n",
    "having correlation <=0.7.\n",
    "\n",
    "(iv) Normalize all the values in the dataset between 0 and 10.\n",
    "\n",
    "(v) Replace all the values in the dataset with 1 if value <=0.5 else with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rB1hsG9GQgE_",
    "outputId": "d49b4e21-3f5b-49d8-fb90-f005f7c0e452"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "rows = 100\n",
    "columns = 30\n",
    "dataset = []\n",
    "for _ in range(rows):\n",
    "    row = [random.randint(1, 200) for _ in range(columns)]\n",
    "    dataset.append(row)\n",
    "df = pd.DataFrame(dataset)\n",
    "# (i)\n",
    "df = df.mask((df >= 10) & (df <= 60), np.nan)\n",
    "count_missing_rows = df.isnull().any(axis=1).sum()\n",
    "print(\"Number of rows with missing values:\", count_missing_rows)\n",
    "# (2)\n",
    "df = df.fillna(df.mean())\n",
    "print(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ba0q43WSl8A9",
    "outputId": "843db356-e34e-4b4b-9c93-a5a23a541bb5"
   },
   "outputs": [],
   "source": [
    "# (3)\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "rows = 100\n",
    "columns = 30\n",
    "dataset = []\n",
    "for _ in range(rows):\n",
    "    row = [random.randint(1, 200) for _ in range(columns)]\n",
    "    dataset.append(row)\n",
    "df = pd.DataFrame(dataset)\n",
    "df = df.mask((df >= 10) & (df <= 60), np.nan)\n",
    "# Calculate the Pearson correlation among all columns\n",
    "correlation_matrix = df.corr()\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"RdYlBu\", vmin=-1, vmax=1)\n",
    "plt.title(\"Pearson Correlation Heatmap\")\n",
    "plt.show()\n",
    "# Select columns with correlation <= 0.7\n",
    "selected_columns = correlation_matrix.columns[correlation_matrix.abs().mean() <= 0.7]\n",
    "print(\"Columns with correlation <= 0.7:\")\n",
    "print(selected_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ohB2Honxmgob",
    "outputId": "d4aca061-a158-481c-b4f2-94fc28aec770"
   },
   "outputs": [],
   "source": [
    "#  (4)\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "rows = 100\n",
    "columns = 30\n",
    "dataset = []\n",
    "for _ in range(rows):\n",
    "    row = [random.randint(1, 200) for _ in range(columns)]\n",
    "    dataset.append(row)\n",
    "df = pd.DataFrame(dataset)\n",
    "# Normalize values between 0 and 10\n",
    "scaler = MinMaxScaler(feature_range=(0, 10))\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(df))\n",
    "# Print the normalized DataFrame\n",
    "print(df_normalized)\n",
    "# (5)\n",
    "# Replace values based on the condition\n",
    "df_normalized = df_normalized.applymap(lambda x: 1 if x <= 0.5 else 0)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "print(df_normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSecOcqeQyNT"
   },
   "source": [
    "Q13. Create a random dataset of 500 rows and 10 columns. \n",
    "\n",
    "Columns 1 to 4 are defined between [-10, 10]; \n",
    "\n",
    "Columns 5 to 8 are defined between [10, 20]; \n",
    "\n",
    "Columns 9 to 10 are defined between [-100, 100]. \n",
    "\n",
    "Apply following clustering algorithms; determine the optimal number of clusters and plot distance metric \n",
    "graph using each algorithm.\n",
    "\n",
    "(i) K-Mean clustering\n",
    "\n",
    "(ii) Hierarchical clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xfk9L8faoPHp",
    "outputId": "db68c5a1-2571-4f30-b59b-fb01e359340e"
   },
   "outputs": [],
   "source": [
    "pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VWqfFaRsoBbn",
    "outputId": "f0c87f21-0967-406b-ffe9-f00f06dba4b8"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import scipy\n",
    "\n",
    "# Set the dimensions of the dataset\n",
    "rows = 500\n",
    "columns = 10\n",
    "\n",
    "# Generate the random dataset\n",
    "dataset = []\n",
    "for _ in range(rows):\n",
    "    row = []\n",
    "    # Columns 1 to 4: [-10, 10]\n",
    "    for _ in range(4):\n",
    "        value = random.uniform(-10, 10)\n",
    "        row.append(value)\n",
    "    # Columns 5 to 8: [10, 20]\n",
    "    for _ in range(4):\n",
    "        value = random.uniform(10, 20)\n",
    "        row.append(value)\n",
    "    # Columns 9 to 10: [-100, 100]\n",
    "    for _ in range(2):\n",
    "        value = random.uniform(-100, 100)\n",
    "        row.append(value)\n",
    "    dataset.append(row)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# K-Means Clustering\n",
    "kmeans_scores = []\n",
    "silhouette_scores = []\n",
    "max_clusters = 10\n",
    "\n",
    "for k in range(2, max_clusters+1):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(df)\n",
    "    labels = kmeans.labels_\n",
    "    kmeans_scores.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(df, labels))\n",
    "\n",
    "# Plotting K-Means clustering results\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(2, max_clusters+1), kmeans_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('K-Means Inertia')\n",
    "plt.title('K-Means Clustering')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(2, max_clusters+1), silhouette_scores, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Hierarchical Clustering\n",
    "distances = scipy.spatial.distance.pdist(df)\n",
    "linkage_matrix = scipy.cluster.hierarchy.linkage(distances, method='ward')\n",
    "\n",
    "# Plotting Hierarchical clustering results\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(linkage_matrix)\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Distance')\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoLcdmSFQ5qy"
   },
   "source": [
    "Q14. Create a random dataset of 600 rows and 15 columns. All the values are defined between [-100,100]. \n",
    "Perform the following operations:\n",
    "\n",
    "(i) Plot scatter graph between Column 5 and Column 6.\n",
    "\n",
    "(ii) Plot histogram of each column in single graph.\n",
    "\n",
    "(iii) Plot the Box plot of each column in single graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vy8TpreaRFzC",
    "outputId": "a0593e0b-5e17-4fb3-95cb-efbabdf23e21"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set the dimensions of the dataset\n",
    "rows = 600\n",
    "columns = 15\n",
    "\n",
    "# Generate the random dataset\n",
    "dataset = []\n",
    "for _ in range(rows):\n",
    "    row = [random.uniform(-100, 100) for _ in range(columns)]\n",
    "    dataset.append(row)\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "\n",
    "# Plot scatter graph between Column 5 and Column 6\n",
    "plt.scatter(df[4], df[5])\n",
    "plt.xlabel('Column 5')\n",
    "plt.ylabel('Column 6')\n",
    "plt.title('Scatter Graph')\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram of each column in a single graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(columns):\n",
    "    plt.hist(df[i], bins=20, alpha=0.5, label='Column {}'.format(i+1))\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Each Column')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot box plot of each column in a single graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(df.values, vert=False, labels=df.columns)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Column')\n",
    "plt.title('Box Plot of Each Column')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbftZv6lQ95u"
   },
   "source": [
    "Q15. Create a random dataset of 500 rows and 5 columns:\n",
    "All the values are defined between [5,10]. \n",
    "\n",
    "Perform the following operations:\n",
    "\n",
    "(i) Perform t-Test on each column.\n",
    "\n",
    "(ii) Perform Wilcoxon Signed Rank Test on each column.\n",
    "\n",
    "(iii) Perform Two Sample t-Test and Wilcoxon Rank Sum Test on Column 3 and Column 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E5mk4-9yRGUl",
    "outputId": "33eb98a2-7126-40db-d0c6-59d0cf6346cf"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Set the dimensions of the dataset\n",
    "rows = 500\n",
    "columns = 5\n",
    "\n",
    "# Generate the random dataset\n",
    "dataset = []\n",
    "for _ in range(rows):\n",
    "    row = [random.uniform(5, 10) for _ in range(columns)]\n",
    "    dataset.append(row)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Perform t-test on each column\n",
    "population_mean = 7.5  # Specify the population mean\n",
    "\n",
    "for col in df.columns:\n",
    "    column_data = df[col]\n",
    "    t_statistic, p_value = stats.ttest_1samp(column_data, population_mean)\n",
    "    print(\"Column {}: t-statistic = {:.4f}, p-value = {:.4f}\".format(col, t_statistic, p_value))\n",
    "\n",
    "# Perform Wilcoxon signed-rank test on each column\n",
    "for col in df.columns:\n",
    "    column_data = df[col]\n",
    "    statistic, p_value = stats.wilcoxon(column_data)\n",
    "    print(\"Column {}: statistic = {:.4f}, p-value = {:.4f}\".format(col, statistic, p_value))\n",
    "\n",
    "# Perform two-sample t-test on Column 3 and Column 4\n",
    "column3_data = df[2]\n",
    "column4_data = df[3]\n",
    "t_statistic, p_value_ttest = stats.ttest_ind(column3_data, column4_data)\n",
    "print(\"Two-sample t-test:\")\n",
    "print(\"t-statistic = {:.4f}, p-value = {:.4f}\".format(t_statistic, p_value_ttest))\n",
    "\n",
    "# Perform Wilcoxon rank sum test on Column 3 and Column 4\n",
    "z_statistic, p_value_wilcoxon = stats.ranksums(column3_data, column4_data)\n",
    "print(\"\\nWilcoxon rank sum test:\")\n",
    "print(\"z-statistic = {:.4f}, p-value = {:.4f}\".format(z_statistic, p_value_wilcoxon))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
